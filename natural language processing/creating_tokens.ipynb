{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ef8cbd",
   "metadata": {},
   "source": [
    "# Video Explanation\n",
    "\n",
    "I creted this\n",
    "\n",
    "[https://www.youtube.com/watch?v=MeY7KCvozOw](https://www.youtube.com/watch?v=MeY7KCvozOw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc82e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tanishqsingh/.cache/kagglehub/datasets/kritanjalijain/amazon-reviews/versions/2\n",
      "/Users/tanishqsingh/.cache/kagglehub/datasets/leadbest/googlenewsvectorsnegative300/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "amazon = kagglehub.dataset_download(\"kritanjalijain/amazon-reviews\")\n",
    "google_vector = kagglehub.dataset_download(\"leadbest/googlenewsvectorsnegative300\")\n",
    "\n",
    "print(amazon)\n",
    "print(google_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48c14a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "columns = ['sentiment', 'title', 'description']\n",
    "df = pd.read_csv(f\"{amazon}/train.csv\", header=None, names=columns, chunksize=15_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5945596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tanishqsingh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tanishqsingh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import contractions\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb11b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "    u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "    u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "    \"]+\", flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def clean_text(df, column):\n",
    "    df[column] = df[column].str.lower()\n",
    "    df[column] = df[column].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "    df[column] = df[column].str.replace(r\"\\d+\", \"\", regex=True)\n",
    "    df[column] = df[column].str.replace(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b\", \"\", regex=True)\n",
    "    df[column] = df[column].apply(lambda x: contractions.fix(x))\n",
    "    df[column] = df[column].apply(lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "    df[column] = df[column].apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "    # df[column] = df[column].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))\n",
    "    \n",
    "    # df[column] = df[column].apply(lambda x: TextBlob(x).correct().string)\n",
    "    df[column] = df[column].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    return df[column]\n",
    "\n",
    "text_file = open('amazon.train.txt', 'w')\n",
    "\n",
    "for idx, chunk in enumerate(df):\n",
    "    print(idx)\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "\n",
    "    text_file.write(\" \".join(clean_text(chunk, 'title').tolist()) + ' ')\n",
    "    text_file.write(\" \".join(clean_text(chunk, 'description').tolist()) + '\\n')\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b97d0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = PathLineSentences(\"amazon.train.txt\")\n",
    "model = Word2Vec(vector_size=300, window=5, workers=multiprocessing.cpu_count() * 2, compute_loss=True, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "55377ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4188ce29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "Loss: 20881514.0\n",
      "Epoch #2\n",
      "Loss: -517128.0\n",
      "Epoch #3\n",
      "Loss: -236060.0\n",
      "Epoch #4\n",
      "Loss: 40044.0\n",
      "Epoch #5\n",
      "Loss: 352.0\n"
     ]
    }
   ],
   "source": [
    "previous_loss = 0\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch #{epoch + 1}\")\n",
    "\n",
    "    model.train(\n",
    "        sentences,\n",
    "        total_examples=model.corpus_count,\n",
    "        epochs=1,\n",
    "        compute_loss=True\n",
    "    )\n",
    "    \n",
    "    current_loss = model.get_latest_training_loss()\n",
    "    print(f\"Loss: {current_loss - previous_loss}\")\n",
    "    previous_loss = current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf003017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v = KeyedVectors.load(\"amazon_vectors.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89bf56ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train_vectors = []\n",
    "y_train_vectors = []\n",
    "\n",
    "for idx, chunk in enumerate(df):\n",
    "    if idx % 20 == 0:\n",
    "        print(idx)\n",
    "\n",
    "    chunk.dropna(inplace=True)\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "\n",
    "    chunk['title'] = clean_text(chunk, 'title')\n",
    "    chunk['description'] = clean_text(chunk, 'description')\n",
    "\n",
    "    x_train_vectors.append(chunk['description'].apply(lambda text: np.mean([w2v.wv[word] for word in text.split() if word in w2v.wv])).tolist())\n",
    "    y_train_vectors.append(chunk['sentiment'] - 1)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55c160b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14999,), (14999,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train_vectors)[0]\n",
    "y_train = np.array(x_train_vectors)[0]\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "546dd38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">385</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m385\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">385</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m385\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(1,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf240c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
