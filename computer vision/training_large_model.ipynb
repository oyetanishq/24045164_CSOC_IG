{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Video Explanation\n",
        "\n",
        "I creted this\n",
        "\n",
        "[https://www.youtube.com/watch?v=7VSuYIizrdA](https://www.youtube.com/watch?v=7VSuYIizrdA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfDiZZ3ONe61",
        "outputId": "c23f071f-74f5-4e9a-9e86-6022c959cadf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/dimensi0n/imagenet-256?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7.15G/7.15G [01:00<00:00, 126MB/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/dimensi0n/imagenet-256/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"dimensi0n/imagenet-256\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA159VO5OUan",
        "outputId": "e10bec21-38cb-40e1-8530-f3c82620d7b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 539826 files belonging to 1000 classes.\n",
            "Using 431861 files for training.\n",
            "Using 107965 files for validation.\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import image_dataset_from_directory\n",
        "\n",
        "\"\"\"\n",
        "split train, and test dataset, there are tensors of 32 images batch\n",
        "each element of this tensor have element[0] = tensor of 32 images with (256,256,3) shape\n",
        "and element[1] = tensor of 32 interger values these are classes those respective image belong\n",
        "\"\"\"\n",
        "(train_dataset_org, test_dataset_org) = image_dataset_from_directory(\n",
        "    directory = path,\n",
        "    validation_split = 0.2,\n",
        "    subset= 'both', # return both train, and test\n",
        "    labels = 'inferred',\n",
        "    label_mode = 'categorical',\n",
        "    batch_size = 500, # how many images to be considered per element of this dataset\n",
        "    image_size = (256, 256),\n",
        "    seed = 123\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt2_VZbdPvm_"
      },
      "outputs": [],
      "source": [
        "from tensorflow import cast, float32\n",
        "\n",
        "# you know these images pixel range from 0 to 255, so you need to scale it down b/w 0 and 1.\n",
        "train_dataset = train_dataset_org.map(lambda x, y: (cast(x/255.0, float32), y))\n",
        "test_dataset = test_dataset_org.map(lambda x, y: (cast(x/255.0, float32), y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJCFarXMP7Kv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "\n",
        "def residual_block(x, filters, kernel_size=3, stride=1):\n",
        "    shortcut = x\n",
        "\n",
        "    # First 3x3 convolution\n",
        "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # Second 3x3 convolution\n",
        "    x = layers.Conv2D(filters, kernel_size, strides=1, padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # If shape/number of channels changed, adjust the shortcut:\n",
        "    if shortcut.shape[-1] != filters:\n",
        "        shortcut = layers.Conv2D(filters, 1, strides=stride, padding='same', use_bias=False)(shortcut)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    # Elementwise add + final ReLU\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    x = layers.ReLU()(x)\n",
        "    return x\n",
        "\n",
        "def inception_block(x, filters):\n",
        "    # Branch 1: 1x1\n",
        "    branch1 = layers.Conv2D(filters // 4, 1, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Branch 2: 1x1\n",
        "    branch2 = layers.Conv2D(filters // 4, 1, padding='same', activation='relu')(x)\n",
        "    branch2 = layers.Conv2D(filters // 4, 3, padding='same', activation='relu')(branch2)\n",
        "\n",
        "    # Branch 3: 1x1\n",
        "    branch3 = layers.Conv2D(filters // 4, 1, padding='same', activation='relu')(x)\n",
        "    branch3 = layers.Conv2D(filters // 4, 5, padding='same', activation='relu')(branch3)\n",
        "\n",
        "    # Branch 4: 3x3\n",
        "    branch4 = layers.MaxPooling2D(3, strides=1, padding='same')(x)\n",
        "    branch4 = layers.Conv2D(filters // 4, 1, padding='same', activation='relu')(branch4)\n",
        "\n",
        "    # Concatenate all branches\n",
        "    return layers.Concatenate()([branch1, branch2, branch3, branch4])\n",
        "\n",
        "def build_cnn(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Initial Conv + Pooling\n",
        "    x = layers.Conv2D(64, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    # 2 Residual Blocks\n",
        "    x = residual_block(x, 64, stride=1)\n",
        "    x = residual_block(x, 128, stride=2)  # downsample here\n",
        "\n",
        "    # One Inception‐style Block\n",
        "    x = inception_block(x, 256)\n",
        "\n",
        "    # Transition layer to reduce channels/dimension\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(128, 1, padding='same', use_bias=False)(x)\n",
        "    x = layers.AveragePooling2D(2, strides=2, padding='same')(x)\n",
        "\n",
        "    # 2 More Residual Blocks\n",
        "    x = residual_block(x, 256, stride=1)\n",
        "    x = residual_block(x, 512, stride=2)  # further downsampling\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PjEhg7rP7Gk"
      },
      "outputs": [],
      "source": [
        "model = build_cnn(input_shape=(256, 256, 3), num_classes=1000)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT1kX15zP7Bp",
        "outputId": "fee5058a-3ef7-41ff-b923-cdbc4d336b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m864/864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1270s\u001b[0m 1s/step - accuracy: 0.0635 - loss: 5.4559 - val_accuracy: 0.1270 - val_loss: 4.6936\n",
            "Epoch 2/3\n",
            "\u001b[1m864/864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1187s\u001b[0m 1s/step - accuracy: 0.2386 - loss: 3.6992 - val_accuracy: 0.2574 - val_loss: 3.5937\n",
            "Epoch 3/3\n",
            "\u001b[1m864/864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1230s\u001b[0m 1s/step - accuracy: 0.3557 - loss: 2.9456 - val_accuracy: 0.2683 - val_loss: 3.6202\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d5454f3ccd0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_dataset, validation_data=test_dataset, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-ZjgBUCQPU3",
        "outputId": "241be16d-ca31-4ea5-e001-7996ce1ce5f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save(\"1000-classes-model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeZH2iNqfABR",
        "outputId": "9db642e5-2c11-4d2b-f8c4-ac1fc892a7b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5\n",
            "\u001b[1m864/864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1189s\u001b[0m 1s/step - accuracy: 0.4358 - loss: 2.4915 - val_accuracy: 0.3129 - val_loss: 3.3009\n",
            "Epoch 5/5\n",
            "\u001b[1m506/864\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m6:47\u001b[0m 1s/step - accuracy: 0.4887 - loss: 2.2011"
          ]
        }
      ],
      "source": [
        "model.fit(train_dataset, validation_data=test_dataset, epochs=5, initial_epoch=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Yk8uGhqgWIH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
