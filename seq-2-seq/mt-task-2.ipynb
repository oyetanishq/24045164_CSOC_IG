{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":446960,"sourceType":"datasetVersion","datasetId":203339},{"sourceId":1483651,"sourceType":"datasetVersion","datasetId":870709}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# task 2: seq2seq machine translation with attention","metadata":{}},{"cell_type":"code","source":"# all the datasets that will be needed\n!ls /kaggle/input/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:40:09.306775Z","iopub.execute_input":"2025-06-21T13:40:09.307376Z","iopub.status.idle":"2025-06-21T13:40:09.433855Z","shell.execute_reply.started":"2025-06-21T13:40:09.307338Z","shell.execute_reply":"2025-06-21T13:40:09.432780Z"}},"outputs":[{"name":"stdout","text":"english-to-french  glove-embeddings\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\ndf_english = pd.read_csv(\"/kaggle/input/english-to-french/small_vocab_en.csv\", sep = '\\t' , names = ['english'])\ndf_french = pd.read_csv(\"/kaggle/input/english-to-french/small_vocab_fr.csv\", sep = '\\t' , names = ['french'])\n\ndf = pd.concat([df_english, df_french], axis=1)\ndf.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:40:11.610373Z","iopub.execute_input":"2025-06-21T13:40:11.610722Z","iopub.status.idle":"2025-06-21T13:40:12.596845Z","shell.execute_reply.started":"2025-06-21T13:40:11.610683Z","shell.execute_reply":"2025-06-21T13:40:12.596176Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                             english  \\\n0  new jersey is sometimes quiet during autumn , ...   \n1  the united states is usually chilly during jul...   \n\n                                              french  \n0  new jersey est parfois calme pendant l' automn...  \n1  les états-unis est généralement froid en juill...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english</th>\n      <th>french</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>new jersey is sometimes quiet during autumn , ...</td>\n      <td>new jersey est parfois calme pendant l' automn...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the united states is usually chilly during jul...</td>\n      <td>les états-unis est généralement froid en juill...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:40:14.197981Z","iopub.execute_input":"2025-06-21T13:40:14.198761Z","iopub.status.idle":"2025-06-21T13:40:14.238481Z","shell.execute_reply.started":"2025-06-21T13:40:14.198735Z","shell.execute_reply":"2025-06-21T13:40:14.237791Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 137860 entries, 0 to 137859\nData columns (total 2 columns):\n #   Column   Non-Null Count   Dtype \n---  ------   --------------   ----- \n 0   english  137860 non-null  object\n 1   french   137860 non-null  object\ndtypes: object(2)\nmemory usage: 2.1+ MB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Data pre-processing/cleaning","metadata":{}},{"cell_type":"code","source":"import string\nimport re\n\npunctuation_pattern = f\"[{re.escape(string.punctuation)}]\" # remove all the punctuations\nprintable_pattern = re.compile(f\"[^{re.escape(string.printable)}]\") # remove all the non-printable characters\n\ndef clean_sentences(sentence):\n    clean = str(sentence)\n    clean = printable_pattern.sub('', clean)\n    clean = re.compile(punctuation_pattern).sub('', clean)\n    \n    return clean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:40:26.561717Z","iopub.execute_input":"2025-06-21T13:40:26.562254Z","iopub.status.idle":"2025-06-21T13:40:26.566896Z","shell.execute_reply.started":"2025-06-21T13:40:26.562229Z","shell.execute_reply":"2025-06-21T13:40:26.566100Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df['english'] = df['english'].astype(str).str.replace(punctuation_pattern, '', regex=True).str.lower().apply(lambda x: printable_pattern.sub('', x)).str.strip()\ndf['french']  =  df['french'].astype(str).str.replace(punctuation_pattern, '', regex=True).str.lower().str.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:40:31.575406Z","iopub.execute_input":"2025-06-21T13:40:31.575708Z","iopub.status.idle":"2025-06-21T13:40:32.384200Z","shell.execute_reply.started":"2025-06-21T13:40:31.575686Z","shell.execute_reply":"2025-06-21T13:40:32.383363Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"maximum_input_length  = 20 # maximum words for input (english)\nmaximum_output_length = 20 # maximum words for output (french)\n\nsentences_en = df['english'].apply(lambda x: \" \".join(x.split(\" \")[:maximum_input_length]))\nsentences_fr =  df['french'].apply(lambda x: \" \".join(x.split(\" \")[:maximum_input_length]))\n\nsentences_fr_input  = df['french'].apply(lambda x: \"<start> \" + \" \".join(x.split(\" \")[:maximum_output_length-1]))\nsentences_fr_output = df['french'].apply(lambda x: \" \".join(x.split(\" \")[:maximum_output_length-1]) + \" <end>\")\n\nprint(f\"cleaned english sentence         : {sentences_en[0]}\")\nprint(f\"cleaned french sentence          : {sentences_fr[0]}\")\nprint(f\"cleaned french (start) sentence  : {sentences_fr_input[0]}\")\nprint(f\"cleaned french (end) sentence    : {sentences_fr_output[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:40:35.248967Z","iopub.execute_input":"2025-06-21T13:40:35.249525Z","iopub.status.idle":"2025-06-21T13:40:35.977877Z","shell.execute_reply.started":"2025-06-21T13:40:35.249500Z","shell.execute_reply":"2025-06-21T13:40:35.977137Z"}},"outputs":[{"name":"stdout","text":"cleaned english sentence         : new jersey is sometimes quiet during autumn  and it is snowy in april\ncleaned french sentence          : new jersey est parfois calme pendant l automne  et il est neigeux en avril\ncleaned french (start) sentence  : <start> new jersey est parfois calme pendant l automne  et il est neigeux en avril\ncleaned french (end) sentence    : new jersey est parfois calme pendant l automne  et il est neigeux en avril <end>\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import  Tokenizer\n\ntokenizer_en = Tokenizer(oov_token='oov')\ntokenizer_fr = Tokenizer(oov_token='oov', filters='')\n\ntokenizer_en.fit_on_texts(sentences_en)\ntokenizer_fr.fit_on_texts(sentences_fr_input)\ntokenizer_fr.fit_on_texts(sentences_fr_output)\n\ntokenized_en = tokenizer_en.texts_to_sequences(sentences_en)\ntokenized_fr_input  = tokenizer_fr.texts_to_sequences(sentences_fr_input)\ntokenized_fr_output = tokenizer_fr.texts_to_sequences(sentences_fr_output)\n\nvocab_en = len(tokenizer_en.word_index) + 1\nvocab_fr = len(tokenizer_fr.word_index) + 1\n\nprint(f\"vocab english : {vocab_en}\")\nprint(f\"vocab hindi   : {vocab_fr}\")\n\nprint(f\"longest english sentence : {maximum_input_length}\")\nprint(f\"longest hindi sentence   : {maximum_output_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:41:04.645853Z","iopub.execute_input":"2025-06-21T13:41:04.646376Z","iopub.status.idle":"2025-06-21T13:41:13.534033Z","shell.execute_reply.started":"2025-06-21T13:41:04.646352Z","shell.execute_reply":"2025-06-21T13:41:13.533261Z"}},"outputs":[{"name":"stdout","text":"vocab english : 201\nvocab hindi   : 348\nlongest english sentence : 20\nlongest hindi sentence   : 20\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# why pre-padding input, and post-padding output?\n\n- as lstm carry states, we want as much data to be preserved, so if we put words at last (pre-padding) then it can remember more.\n- for output we need to generate a sequence of words from left to right, hence forcing to learn (post-padding), useful words at start","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\npadded_en = pad_sequences(tokenized_en, maxlen=maximum_input_length, padding='pre')\n\npadded_fr_input  = pad_sequences(tokenized_fr_input, maxlen=maximum_output_length, padding='post')\npadded_fr_output = pad_sequences(tokenized_fr_output, maxlen=maximum_output_length, padding='post')\n\nprint(repr(f\"english padded       : {padded_en[0]}\"))\nprint(repr(f\"french input padded  : {padded_fr_input[0]}\"))\nprint(repr(f\"french output padded : {padded_fr_output[0]}\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:41:13.535257Z","iopub.execute_input":"2025-06-21T13:41:13.535542Z","iopub.status.idle":"2025-06-21T13:41:14.381447Z","shell.execute_reply.started":"2025-06-21T13:41:13.535525Z","shell.execute_reply":"2025-06-21T13:41:14.380714Z"}},"outputs":[{"name":"stdout","text":"'english padded       : [ 0  0  0  0  0  0  0 18 24  2  9 68  5 40  8  4  2 56  3 45]'\n'french input padded  : [  5  37  36   2  11  69  39  14  27   9   4   2 114   3  52   0   0   0\\n   0   0]'\n'french output padded : [ 37  36   2  11  69  39  14  27   9   4   2 114   3  52   6   0   0   0\\n   0   0]'\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Train-Test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train_en, x_test_en, x_train_fr, x_test_fr, y_train_fr, y_test_fr = train_test_split(\n    padded_en, padded_fr_input, padded_fr_output,\n    test_size=0.2\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:41:14.382468Z","iopub.execute_input":"2025-06-21T13:41:14.382784Z","iopub.status.idle":"2025-06-21T13:41:14.561344Z","shell.execute_reply.started":"2025-06-21T13:41:14.382759Z","shell.execute_reply":"2025-06-21T13:41:14.560523Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# this contains the representation of words in 200 dimension vector\n- each word can be represented in vec of 200 values\n- storing in a `glove_embedding` dict","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nglove_embedding = dict()\n\nwith open(\"/kaggle/input/glove-embeddings/glove.6B.200d.txt\", encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        \n        word = values[0]                   # word\n        vectors = np.asarray(values[1:])   # 200 dim vector representation of that word\n        \n        glove_embedding[word] = vectors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:41:16.743821Z","iopub.execute_input":"2025-06-21T13:41:16.744093Z","iopub.status.idle":"2025-06-21T13:41:40.465794Z","shell.execute_reply.started":"2025-06-21T13:41:16.744072Z","shell.execute_reply":"2025-06-21T13:41:40.465181Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# if the vocab words from training set is found then update the embedding_matrix from glove_embedding\nembedding_matrix = np.zeros((vocab_en, 200))\n\nfor word, index in tokenizer_en.word_index.items():\n    vector = glove_embedding.get(word)\n    \n    if vector is not None:\n        embedding_matrix[index] = vector","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:41:40.466877Z","iopub.execute_input":"2025-06-21T13:41:40.467119Z","iopub.status.idle":"2025-06-21T13:41:40.484123Z","shell.execute_reply.started":"2025-06-21T13:41:40.467093Z","shell.execute_reply":"2025-06-21T13:41:40.483507Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Model architecture\n- **encoder module**: takes the input words and create states with sentence meaning\n- **decoder module**: takes that states, and for a special token `<start>` it produces a sequence of words, and end with `<end>`","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Embedding, LSTM\n\n\"\"\"\nEncoder\n\n    set maximum input size,\n    set embedding_matrix for the words we found in the glove_embedding as weights in embedding layer,\n    pass the data to lstm layer, which calculates the states, needed for decoder module\n\"\"\"\nencoder_inputs = Input(shape=(maximum_input_length,), name=\"encoder_input\")\nencoder_embedding = Embedding(input_dim=vocab_en, output_dim=200, weights=[embedding_matrix], input_length=maximum_input_length, name=\"encoder_embedding\")(encoder_inputs)\nencoder_outputs, state_h, state_c = LSTM(256, return_sequences=True, return_state=True, name=\"encoder_lstm\")(encoder_embedding)\nencoder_states = [state_h, state_c]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:47:41.139630Z","iopub.execute_input":"2025-06-21T13:47:41.140114Z","iopub.status.idle":"2025-06-21T13:47:41.188844Z","shell.execute_reply.started":"2025-06-21T13:47:41.140091Z","shell.execute_reply":"2025-06-21T13:47:41.188052Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\"\"\"\nDecoder\n\n    set maximum input size,\n    data travels from input -> embedding where each word is representing in 200 dim -> lstm\n    where lstm returns sequence of words, which is then pass through a dence layer to get words probability\n\"\"\"\ndecoder_inputs = Input(shape=(maximum_output_length,), name=\"decoder_input\")\ndecoder_embedding = Embedding(input_dim=vocab_fr, output_dim=200, input_length=maximum_output_length, name=\"decoder_embedding\")(decoder_inputs)\ndecoder_lstm_outputs, _, _ = LSTM(256, return_sequences=True, return_state=True, name=\"decoder_lstm\")(decoder_embedding, initial_state=encoder_states)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:47:42.581699Z","iopub.execute_input":"2025-06-21T13:47:42.581999Z","iopub.status.idle":"2025-06-21T13:47:42.627642Z","shell.execute_reply.started":"2025-06-21T13:47:42.581978Z","shell.execute_reply":"2025-06-21T13:47:42.626694Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from tensorflow.keras.layers import Attention, Concatenate, TimeDistributed, Dense\nfrom tensorflow.keras.models import Model\n\n\"\"\"\nLuong Attention\n\n    what attention does?\n        - for each word that decoder generates, it can look for entine states history in encoder module, and not just last states\n        - hence each word from input has some weight at producing words in decoder module\n\n    each word generated in decoder module is influenced by (timed incoder states), and (prev generated decoder word).\n    hence a context_vector is generates with those two values\n    \n    down code is used to calculate weightage of each word (input) to generate new word (decoder).\n\n    in luong attention :- you take dot product of those two values to get weight\n    in bahdanau attention :- you add those values, along with calculating weight (little more work for training)\n\"\"\"\nattention = Attention(name=\"luong_attention\")\ncontext_vector = attention([decoder_lstm_outputs, encoder_outputs])\ndecoder_combined_context = Concatenate(axis=-1, name=\"attention_concatenate\")([context_vector, decoder_lstm_outputs])\n\n# Output layer\noutput = TimeDistributed(Dense(vocab_fr, activation=\"softmax\"), name=\"decoder_output\")(decoder_combined_context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:56:42.063685Z","iopub.execute_input":"2025-06-21T13:56:42.064008Z","iopub.status.idle":"2025-06-21T13:56:42.080646Z","shell.execute_reply.started":"2025-06-21T13:56:42.063987Z","shell.execute_reply":"2025-06-21T13:56:42.080093Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Define the model\nmodel = Model([encoder_inputs, decoder_inputs], output)\n\n# why sparse categorical cross entropy, as the target output data is not one hot encoded\nmodel.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:56:43.369348Z","iopub.execute_input":"2025-06-21T13:56:43.370023Z","iopub.status.idle":"2025-06-21T13:56:43.394899Z","shell.execute_reply.started":"2025-06-21T13:56:43.370000Z","shell.execute_reply":"2025-06-21T13:56:43.394355Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ encoder_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m200\u001b[0m)        │         \u001b[38;5;34m40,200\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m200\u001b[0m)        │         \u001b[38;5;34m69,600\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │        \u001b[38;5;34m467,968\u001b[0m │ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │                        │\n│                           │ \u001b[38;5;34m256\u001b[0m)]                  │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │        \u001b[38;5;34m467,968\u001b[0m │ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n│                           │ \u001b[38;5;34m256\u001b[0m)]                  │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ luong_attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n│ (\u001b[38;5;33mAttention\u001b[0m)               │                        │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ attention_concatenate     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ luong_attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_output            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m348\u001b[0m)        │        \u001b[38;5;34m178,524\u001b[0m │ attention_concatenate… │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ encoder_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">40,200</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">69,600</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">467,968</span> │ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │                        │\n│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                  │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">467,968</span> │ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                  │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ luong_attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)               │                        │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ attention_concatenate     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ luong_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_output            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">348</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">178,524</span> │ attention_concatenate… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,224,260\u001b[0m (4.67 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,224,260</span> (4.67 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,224,260\u001b[0m (4.67 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,224,260</span> (4.67 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"history = model.fit(\n    # train data\n    [x_train_en, x_train_fr], y_train_fr,\n\n    # test data\n    validation_data=([x_test_en, x_test_fr], y_test_fr),\n\n    # batch size (when to update weights), epochs\n    batch_size=16, epochs=2\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T13:57:43.505225Z","iopub.execute_input":"2025-06-21T13:57:43.505830Z","iopub.status.idle":"2025-06-21T14:00:08.876281Z","shell.execute_reply.started":"2025-06-21T13:57:43.505807Z","shell.execute_reply":"2025-06-21T14:00:08.875633Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/2\n\u001b[1m6893/6893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 10ms/step - accuracy: 0.9671 - loss: 0.1670 - val_accuracy: 0.9927 - val_loss: 0.0242\nEpoch 2/2\n\u001b[1m6893/6893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 10ms/step - accuracy: 0.9935 - loss: 0.0212 - val_accuracy: 0.9940 - val_loss: 0.0197\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# creating a prediction model","metadata":{}},{"cell_type":"code","source":"\"\"\"\nEncoder\n    same as above encoder,\n    takes input padded, tokenized vector, and produces states\n\"\"\"\nencoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c], name=\"encoder_input\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nDecoder\n\n    takes input states of encoder module,\n    and a starting token <start>\n\n    then it produces sequence of words based on (current state + next predicted token)\n    till it receives a special token <end> or reached it's maximum output length, then it stops\n\"\"\"\ndecoder_state_input_h = Input(shape=(256,))\ndecoder_state_input_c = Input(shape=(256,))\ndecoder_hidden_state_input = Input(shape=(maximum_input_length, 256))\n\ndecoder_single_input = Input(shape=(1,))\ndecoder_single_embed = model.get_layer(\"decoder_embedding\")(decoder_single_input)\n\ndecoder_outputs, dec_h, dec_c = model.get_layer(\"decoder_lstm\")( decoder_single_embed, initial_state=[decoder_state_input_h, decoder_state_input_c])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:02:25.860809Z","iopub.execute_input":"2025-06-21T14:02:25.861549Z","iopub.status.idle":"2025-06-21T14:02:25.869567Z","shell.execute_reply.started":"2025-06-21T14:02:25.861522Z","shell.execute_reply":"2025-06-21T14:02:25.868757Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"\"\"\"\nAttention\n\n    each word generated in decoder module is influenced by (timed incoder states), and (prev generated decoder word).\n    hence a context_vector is generates with those two values\n    \n    down code is used to calculate weightage of each word (input) to generate new word (decoder).\n\"\"\"\ncontext = model.get_layer(\"luong_attention\")([decoder_outputs, decoder_hidden_state_input])\nconcat = model.get_layer(\"attention_concatenate\")([context, decoder_outputs])\noutput_tokens = model.get_layer(\"decoder_output\")(concat)\n\ndecoder_model = Model(\n    [decoder_single_input, decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n    [output_tokens, dec_h, dec_c]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:03:31.488181Z","iopub.execute_input":"2025-06-21T14:03:31.488754Z","iopub.status.idle":"2025-06-21T14:03:31.496249Z","shell.execute_reply.started":"2025-06-21T14:03:31.488724Z","shell.execute_reply":"2025-06-21T14:03:31.495372Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def build_idx2word(tokenizer):\n    return {v: k for k, v in tokenizer.word_index.items()}\n\nidx2word_input  = build_idx2word(tokenizer_en)\nidx2word_output = build_idx2word(tokenizer_fr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:03:35.112376Z","iopub.execute_input":"2025-06-21T14:03:35.112652Z","iopub.status.idle":"2025-06-21T14:03:35.116758Z","shell.execute_reply.started":"2025-06-21T14:03:35.112632Z","shell.execute_reply":"2025-06-21T14:03:35.116105Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"\"\"\"\n    takes the sentence, cleans it, tokenize it, pad it,\n    then calculate the sentence state from encoder_module,\n\n    state is then sent to decoder module with a token <start>\n    then this repeats for maximum length of output sentence time, or is <end> token is received\n\"\"\"\ndef translate_english_to_french(input_seq):\n    seq = clean_sentences(input_seq)\n    seq = tokenizer_en.texts_to_sequences([seq])\n    seq = pad_sequences(seq, maxlen=maximum_input_length)\n    \n    enc_outs, h, c = encoder_model.predict(seq, verbose=False)\n\n    target_seq = np.zeros((1, 1))\n    target_seq[0, 0] = tokenizer_fr.word_index[\"<start>\"]\n    \n    eos = tokenizer_fr.texts_to_sequences(['<end>'])[0][0]\n    output_sentence = []\n\n    for _ in range(maximum_output_length):\n        output_tokens, h, c = decoder_model.predict([target_seq, enc_outs, h, c], verbose=False)\n        idx = np.argmax(output_tokens[0, 0, :])\n\n        if idx == eos:\n            break\n            \n        if idx > 0:\n            word = idx2word_output[idx]\n            output_sentence.append(word)\n\n        target_seq[0, 0] = idx\n\n    return \" \".join(output_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:08:51.467761Z","iopub.execute_input":"2025-06-21T14:08:51.468485Z","iopub.status.idle":"2025-06-21T14:08:51.474253Z","shell.execute_reply.started":"2025-06-21T14:08:51.468460Z","shell.execute_reply":"2025-06-21T14:08:51.473576Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"english_sentence = \"california is usually quiet during march  and it is usually hot in june\"\ntranslation = translate_english_to_french(english_sentence)\n\nprint(f\"english (given)     : {english_sentence}\")\nprint(f\"french (prediction) : {translation}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:09:00.459428Z","iopub.execute_input":"2025-06-21T14:09:00.459703Z","iopub.status.idle":"2025-06-21T14:09:01.856018Z","shell.execute_reply.started":"2025-06-21T14:09:00.459686Z","shell.execute_reply":"2025-06-21T14:09:01.855384Z"}},"outputs":[{"name":"stdout","text":"english (given)     : california is usually quiet during march  and it is usually hot in june\nfrench (prediction) : california est généralement calme en mars et il est généralement chaud en juin\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"# belu score","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:09:12.427120Z","iopub.execute_input":"2025-06-21T14:09:12.427690Z","iopub.status.idle":"2025-06-21T14:09:17.950337Z","shell.execute_reply.started":"2025-06-21T14:09:12.427668Z","shell.execute_reply":"2025-06-21T14:09:17.949606Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.2.0 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# some random sentences from data\n\nsentences_idx = np.random.choice(sentences_en.index, size=100)\n\nactual_en = sentences_en[sentences_idx]\nactual_fr = sentences_fr[sentences_idx]\npredicted_fr = [translate_english_to_french(sentence) for sentence in actual_en]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:09:20.840359Z","iopub.execute_input":"2025-06-21T14:09:20.841142Z","iopub.status.idle":"2025-06-21T14:11:35.487110Z","shell.execute_reply.started":"2025-06-21T14:09:20.841108Z","shell.execute_reply":"2025-06-21T14:11:35.486302Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"import sacrebleu\n\nbleu = sacrebleu.corpus_bleu(predicted_fr, [actual_fr.to_list()])\nprint(f\"belu score: {bleu.score:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:11:35.488692Z","iopub.execute_input":"2025-06-21T14:11:35.489067Z","iopub.status.idle":"2025-06-21T14:11:35.603059Z","shell.execute_reply.started":"2025-06-21T14:11:35.489041Z","shell.execute_reply":"2025-06-21T14:11:35.602362Z"}},"outputs":[{"name":"stdout","text":"belu score: 98.16\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}