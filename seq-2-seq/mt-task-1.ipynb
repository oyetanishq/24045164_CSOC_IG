{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 1: seq2seq machine translation without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T12:33:33.092517Z",
     "iopub.status.busy": "2025-06-21T12:33:33.092260Z",
     "iopub.status.idle": "2025-06-21T12:33:33.212105Z",
     "shell.execute_reply": "2025-06-21T12:33:33.211321Z",
     "shell.execute_reply.started": "2025-06-21T12:33:33.092489Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english-to-french  glove-embeddings\n"
     ]
    }
   ],
   "source": [
    "# all the datasets that will be needed\n",
    "!ls /kaggle/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T12:33:34.981114Z",
     "iopub.status.busy": "2025-06-21T12:33:34.980829Z",
     "iopub.status.idle": "2025-06-21T12:33:36.237652Z",
     "shell.execute_reply": "2025-06-21T12:33:36.236909Z",
     "shell.execute_reply.started": "2025-06-21T12:33:34.981087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new jersey is sometimes quiet during autumn , ...</td>\n",
       "      <td>new jersey est parfois calme pendant l' automn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the united states is usually chilly during jul...</td>\n",
       "      <td>les états-unis est généralement froid en juill...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  new jersey is sometimes quiet during autumn , ...   \n",
       "1  the united states is usually chilly during jul...   \n",
       "\n",
       "                                              french  \n",
       "0  new jersey est parfois calme pendant l' automn...  \n",
       "1  les états-unis est généralement froid en juill...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_english = pd.read_csv(\"/kaggle/input/english-to-french/small_vocab_en.csv\", sep = '\\t' , names = ['english'])\n",
    "df_french = pd.read_csv(\"/kaggle/input/english-to-french/small_vocab_fr.csv\", sep = '\\t' , names = ['french'])\n",
    "\n",
    "df = pd.concat([df_english, df_french], axis=1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T12:33:38.378564Z",
     "iopub.status.busy": "2025-06-21T12:33:38.377855Z",
     "iopub.status.idle": "2025-06-21T12:33:38.441343Z",
     "shell.execute_reply": "2025-06-21T12:33:38.440629Z",
     "shell.execute_reply.started": "2025-06-21T12:33:38.378524Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 137860 entries, 0 to 137859\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   english  137860 non-null  object\n",
      " 1   french   137860 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing/cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T12:55:06.037990Z",
     "iopub.status.busy": "2025-06-21T12:55:06.037389Z",
     "iopub.status.idle": "2025-06-21T12:55:06.041990Z",
     "shell.execute_reply": "2025-06-21T12:55:06.041396Z",
     "shell.execute_reply.started": "2025-06-21T12:55:06.037965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "punctuation_pattern = f\"[{re.escape(string.punctuation)}]\" # remove all the punctuations\n",
    "printable_pattern = re.compile(f\"[^{re.escape(string.printable)}]\") # remove all the non-printable characters\n",
    "\n",
    "def clean_sentences(sentence):\n",
    "    clean = str(sentence)\n",
    "    clean = printable_pattern.sub('', clean)\n",
    "    clean = re.compile(punctuation_pattern).sub('', clean)\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T12:55:09.674844Z",
     "iopub.status.busy": "2025-06-21T12:55:09.674185Z",
     "iopub.status.idle": "2025-06-21T12:55:10.211369Z",
     "shell.execute_reply": "2025-06-21T12:55:10.210801Z",
     "shell.execute_reply.started": "2025-06-21T12:55:09.674808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['english'] = df['english'].astype(str).str.replace(punctuation_pattern, '', regex=True).str.lower().apply(lambda x: printable_pattern.sub('', x)).str.strip()\n",
    "df['french']  =  df['french'].astype(str).str.replace(punctuation_pattern, '', regex=True).str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:12:27.221248Z",
     "iopub.status.busy": "2025-06-21T13:12:27.220419Z",
     "iopub.status.idle": "2025-06-21T13:12:27.970575Z",
     "shell.execute_reply": "2025-06-21T13:12:27.969862Z",
     "shell.execute_reply.started": "2025-06-21T13:12:27.221218Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned english sentence         : new jersey is sometimes quiet during autumn  and it is snowy in april\n",
      "cleaned french sentence          : new jersey est parfois calme pendant l automne  et il est neigeux en avril\n",
      "cleaned french (start) sentence  : <start> new jersey est parfois calme pendant l automne  et il est neigeux en avril\n",
      "cleaned french (end) sentence    : new jersey est parfois calme pendant l automne  et il est neigeux en avril <end>\n"
     ]
    }
   ],
   "source": [
    "maximum_input_length  = 20 # maximum words for input (english)\n",
    "maximum_output_length = 20 # maximum words for output (french)\n",
    "\n",
    "sentences_en = df['english'].apply(lambda x: \" \".join(x.split(\" \")[:maximum_input_length]))\n",
    "sentences_fr =  df['french'].apply(lambda x: \" \".join(x.split(\" \")[:maximum_input_length]))\n",
    "\n",
    "sentences_fr_input  = df['french'].apply(lambda x: \"<start> \" + \" \".join(x.split(\" \")[:maximum_output_length-1]))\n",
    "sentences_fr_output = df['french'].apply(lambda x: \" \".join(x.split(\" \")[:maximum_output_length-1]) + \" <end>\")\n",
    "\n",
    "print(f\"cleaned english sentence         : {sentences_en[0]}\")\n",
    "print(f\"cleaned french sentence          : {sentences_fr[0]}\")\n",
    "print(f\"cleaned french (start) sentence  : {sentences_fr_input[0]}\")\n",
    "print(f\"cleaned french (end) sentence    : {sentences_fr_output[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T12:34:37.852221Z",
     "iopub.status.busy": "2025-06-21T12:34:37.851234Z",
     "iopub.status.idle": "2025-06-21T12:34:46.877071Z",
     "shell.execute_reply": "2025-06-21T12:34:46.876375Z",
     "shell.execute_reply.started": "2025-06-21T12:34:37.852195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab english : 201\n",
      "vocab hindi   : 348\n",
      "longest english sentence : 20\n",
      "longest hindi sentence   : 20\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import  Tokenizer\n",
    "\n",
    "tokenizer_en = Tokenizer(oov_token='oov')\n",
    "tokenizer_fr = Tokenizer(oov_token='oov', filters='')\n",
    "\n",
    "tokenizer_en.fit_on_texts(sentences_en)\n",
    "tokenizer_fr.fit_on_texts(sentences_fr_input)\n",
    "tokenizer_fr.fit_on_texts(sentences_fr_output)\n",
    "\n",
    "tokenized_en = tokenizer_en.texts_to_sequences(sentences_en)\n",
    "tokenized_fr_input  = tokenizer_fr.texts_to_sequences(sentences_fr_input)\n",
    "tokenized_fr_output = tokenizer_fr.texts_to_sequences(sentences_fr_output)\n",
    "\n",
    "vocab_en = len(tokenizer_en.word_index) + 1\n",
    "vocab_fr = len(tokenizer_fr.word_index) + 1\n",
    "\n",
    "print(f\"vocab english : {vocab_en}\")\n",
    "print(f\"vocab hindi   : {vocab_fr}\")\n",
    "\n",
    "print(f\"longest english sentence : {maximum_input_length}\")\n",
    "print(f\"longest hindi sentence   : {maximum_output_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why pre-padding input, and post-padding output?\n",
    "\n",
    "- as lstm carry states, we want as much data to be preserved, so if we put words at last (pre-padding) then it can remember more.\n",
    "- for output we need to generate a sequence of words from left to right, hence forcing to learn (post-padding), useful words at start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T12:34:49.426845Z",
     "iopub.status.busy": "2025-06-21T12:34:49.426109Z",
     "iopub.status.idle": "2025-06-21T12:34:50.336815Z",
     "shell.execute_reply": "2025-06-21T12:34:50.336154Z",
     "shell.execute_reply.started": "2025-06-21T12:34:49.426819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'english padded       : [ 0  0  0  0  0  0  0 18 24  2  9 68  5 40  8  4  2 56  3 45]'\n",
      "'french input padded  : [  5  37  36   2  11  69  39  14  27   9   4   2 114   3  52   0   0   0\\n   0   0]'\n",
      "'french output padded : [ 37  36   2  11  69  39  14  27   9   4   2 114   3  52   6   0   0   0\\n   0   0]'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_en = pad_sequences(tokenized_en, maxlen=maximum_input_length, padding='pre')\n",
    "\n",
    "padded_fr_input  = pad_sequences(tokenized_fr_input, maxlen=maximum_output_length, padding='post')\n",
    "padded_fr_output = pad_sequences(tokenized_fr_output, maxlen=maximum_output_length, padding='post')\n",
    "\n",
    "print(repr(f\"english padded       : {padded_en[0]}\"))\n",
    "print(repr(f\"french input padded  : {padded_fr_input[0]}\"))\n",
    "print(repr(f\"french output padded : {padded_fr_output[0]}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:00:48.554655Z",
     "iopub.status.busy": "2025-06-21T13:00:48.553966Z",
     "iopub.status.idle": "2025-06-21T13:00:48.593133Z",
     "shell.execute_reply": "2025-06-21T13:00:48.592576Z",
     "shell.execute_reply.started": "2025-06-21T13:00:48.554604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_en, x_test_en, x_train_fr, x_test_fr, y_train_fr, y_test_fr = train_test_split(\n",
    "    padded_en, padded_fr_input, padded_fr_output,\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this contains the representation of words in 200 dimension vector\n",
    "- each word can be represented in vec of 200 values\n",
    "- storing in a `glove_embedding` dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T12:34:53.303080Z",
     "iopub.status.busy": "2025-06-21T12:34:53.302534Z",
     "iopub.status.idle": "2025-06-21T12:35:18.231035Z",
     "shell.execute_reply": "2025-06-21T12:35:18.230415Z",
     "shell.execute_reply.started": "2025-06-21T12:34:53.303058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "glove_embedding = dict()\n",
    "\n",
    "with open(\"/kaggle/input/glove-embeddings/glove.6B.200d.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        \n",
    "        word = values[0]                   # word\n",
    "        vectors = np.asarray(values[1:])   # 200 dim vector representation of that word\n",
    "        \n",
    "        glove_embedding[word] = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T12:35:18.232613Z",
     "iopub.status.busy": "2025-06-21T12:35:18.232380Z",
     "iopub.status.idle": "2025-06-21T12:35:18.250342Z",
     "shell.execute_reply": "2025-06-21T12:35:18.249657Z",
     "shell.execute_reply.started": "2025-06-21T12:35:18.232595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# if the vocab words from training set is found then update the embedding_matrix from glove_embedding\n",
    "embedding_matrix = np.zeros((vocab_en, 200))\n",
    "\n",
    "for word, index in tokenizer_en.word_index.items():\n",
    "    vector = glove_embedding.get(word)\n",
    "    \n",
    "    if vector is not None:\n",
    "        embedding_matrix[index] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "- **encoder module**: takes the input words and create states with sentence meaning\n",
    "- **decoder module**: takes that states, and for a special token `<start>` it produces a sequence of words, and end with `<end>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:03:54.278452Z",
     "iopub.status.busy": "2025-06-21T13:03:54.278160Z",
     "iopub.status.idle": "2025-06-21T13:03:54.322824Z",
     "shell.execute_reply": "2025-06-21T13:03:54.322077Z",
     "shell.execute_reply.started": "2025-06-21T13:03:54.278431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Input, LSTM\n",
    "\n",
    "\"\"\"\n",
    "Encoder\n",
    "\n",
    "    set maximum input size,\n",
    "    set embedding_matrix for the words we found in the glove_embedding as weights in embedding layer,\n",
    "    pass the data to lstm layer, which calculates the states, needed for decoder module\n",
    "\"\"\"\n",
    "encoder_inputs = Input(shape=(maximum_input_length,), name=\"encoder_input\")\n",
    "embedding_layer = Embedding(vocab_en, 200, weights=[embedding_matrix], input_length=maximum_input_length, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs, h, c = LSTM(256, return_state=True, name=\"encoder_lstm\")(embedding_layer)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:03:56.379925Z",
     "iopub.status.busy": "2025-06-21T13:03:56.379652Z",
     "iopub.status.idle": "2025-06-21T13:03:56.427310Z",
     "shell.execute_reply": "2025-06-21T13:03:56.426657Z",
     "shell.execute_reply.started": "2025-06-21T13:03:56.379905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\"\"\"\n",
    "Decoder\n",
    "\n",
    "    set maximum input size,\n",
    "    data travels from input -> embedding where each word is representing in 200 dim -> lstm\n",
    "    where lstm returns sequence of words, which is then pass through a dence layer to get words probability\n",
    "\"\"\"\n",
    "decoder_inputs = Input(shape=(maximum_output_length,), name=\"decoder_input\")\n",
    "decoder_embedding = Embedding(vocab_fr, 200, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_outputs, _, _ = LSTM(256, return_sequences=True, return_state=True, name=\"decoder_lstm\")(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_outputs = Dense(vocab_fr, activation='softmax', name=\"decoder_dense\")(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:03:58.550335Z",
     "iopub.status.busy": "2025-06-21T13:03:58.550059Z",
     "iopub.status.idle": "2025-06-21T13:03:58.573835Z",
     "shell.execute_reply": "2025-06-21T13:03:58.573126Z",
     "shell.execute_reply.started": "2025-06-21T13:03:58.550314Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ encoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">40,200</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">69,600</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">467,968</span> │ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">467,968</span> │ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n",
       "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                  │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">348</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">89,436</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ encoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m200\u001b[0m)        │         \u001b[38;5;34m40,200\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m200\u001b[0m)        │         \u001b[38;5;34m69,600\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │        \u001b[38;5;34m467,968\u001b[0m │ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                           │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │        \u001b[38;5;34m467,968\u001b[0m │ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n",
       "│                           │ \u001b[38;5;34m256\u001b[0m)]                  │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_dense (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m348\u001b[0m)        │         \u001b[38;5;34m89,436\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,135,172</span> (4.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,135,172\u001b[0m (4.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,135,172</span> (4.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,135,172\u001b[0m (4.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# why sparse categorical cross entropy, as the target output data is not one hot encoded\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:04:45.289754Z",
     "iopub.status.busy": "2025-06-21T13:04:45.289415Z",
     "iopub.status.idle": "2025-06-21T13:06:27.144466Z",
     "shell.execute_reply": "2025-06-21T13:06:27.143872Z",
     "shell.execute_reply.started": "2025-06-21T13:04:45.289730Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m6893/6893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 7ms/step - accuracy: 0.8013 - loss: 0.7479 - val_accuracy: 0.9868 - val_loss: 0.0474\n",
      "Epoch 2/2\n",
      "\u001b[1m6893/6893\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 7ms/step - accuracy: 0.9894 - loss: 0.0368 - val_accuracy: 0.9925 - val_loss: 0.0261\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    # train data\n",
    "    [x_train_en, x_train_fr], y_train_fr,\n",
    "\n",
    "    # test data\n",
    "    validation_data=([x_test_en, x_test_fr], y_test_fr),\n",
    "\n",
    "    # batch size (when to update weights), epochs\n",
    "    batch_size=16, epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:06:37.165396Z",
     "iopub.status.busy": "2025-06-21T13:06:37.165143Z",
     "iopub.status.idle": "2025-06-21T13:06:37.171141Z",
     "shell.execute_reply": "2025-06-21T13:06:37.170527Z",
     "shell.execute_reply.started": "2025-06-21T13:06:37.165377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encoder\n",
    "\n",
    "    same as encoder module above, takes words input,\n",
    "    and produces states which represent meaning of sentence\n",
    "\"\"\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:06:37.358850Z",
     "iopub.status.busy": "2025-06-21T13:06:37.358609Z",
     "iopub.status.idle": "2025-06-21T13:06:37.369880Z",
     "shell.execute_reply": "2025-06-21T13:06:37.369349Z",
     "shell.execute_reply.started": "2025-06-21T13:06:37.358834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decoder\n",
    "\n",
    "    takes input states of encoder module,\n",
    "    and a starting token <start>\n",
    "\n",
    "    then it produces sequence of words based on (current state + next predicted token)\n",
    "    till it receives a special token <end> or reached it's maximum output length, then it stops\n",
    "\"\"\"\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = model.get_layer(\"decoder_embedding\")(decoder_inputs_single)\n",
    "decoder_outputs, h, c = model.get_layer(\"decoder_lstm\")(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "decoder_states = [h, c]\n",
    "\n",
    "decoder_outputs = model.get_layer(\"decoder_dense\")(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:06:43.429050Z",
     "iopub.status.busy": "2025-06-21T13:06:43.428469Z",
     "iopub.status.idle": "2025-06-21T13:06:43.432938Z",
     "shell.execute_reply": "2025-06-21T13:06:43.432344Z",
     "shell.execute_reply.started": "2025-06-21T13:06:43.429026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_idx2word(tokenizer):\n",
    "    return {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "idx2word_input  = build_idx2word(tokenizer_en)\n",
    "idx2word_output = build_idx2word(tokenizer_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:30:51.743174Z",
     "iopub.status.busy": "2025-06-21T13:30:51.742911Z",
     "iopub.status.idle": "2025-06-21T13:30:51.749865Z",
     "shell.execute_reply": "2025-06-21T13:30:51.749056Z",
     "shell.execute_reply.started": "2025-06-21T13:30:51.743155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    takes the sentence, cleans it, tokenize it, pad it,\n",
    "    then calculate the sentence state from encoder_module,\n",
    "\n",
    "    state is then sent to decoder module with a token <start>\n",
    "    then this repeats for maximum length of output sentence time, or is <end> token is received\n",
    "\"\"\"\n",
    "def translate_english_to_french(input_seq):\n",
    "    seq = clean_sentences(input_seq)\n",
    "    seq = tokenizer_en.texts_to_sequences([seq])\n",
    "    seq = pad_sequences(seq, maxlen=maximum_input_length)\n",
    "    \n",
    "    states_value = encoder_model.predict(seq, verbose=False)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer_fr.word_index['<start>']\n",
    "    \n",
    "    eos = tokenizer_fr.texts_to_sequences(['<end>'])[0][0]\n",
    "    \n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(maximum_input_length):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=False)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "        \n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        if idx > 0:\n",
    "            word = idx2word_output[idx]\n",
    "            output_sentence.append(word)\n",
    "            \n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return \" \".join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:06:48.199461Z",
     "iopub.status.busy": "2025-06-21T13:06:48.199180Z",
     "iopub.status.idle": "2025-06-21T13:06:50.136739Z",
     "shell.execute_reply": "2025-06-21T13:06:50.136035Z",
     "shell.execute_reply.started": "2025-06-21T13:06:48.199441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english (given)     : california is usually quiet during march  and it is usually hot in june\n",
      "french (prediction) : california est généralement calme en mars et il est généralement chaud en juin\n"
     ]
    }
   ],
   "source": [
    "english_sentence = \"california is usually quiet during march  and it is usually hot in june\"\n",
    "translation = translate_english_to_french(english_sentence)\n",
    "\n",
    "print(f\"english (given)     : {english_sentence}\")\n",
    "print(f\"french (prediction) : {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# belu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:08:20.572628Z",
     "iopub.status.busy": "2025-06-21T13:08:20.572081Z",
     "iopub.status.idle": "2025-06-21T13:08:28.848356Z",
     "shell.execute_reply": "2025-06-21T13:08:28.847669Z",
     "shell.execute_reply.started": "2025-06-21T13:08:20.572592Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-3.2.0 sacrebleu-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:33:13.447773Z",
     "iopub.status.busy": "2025-06-21T13:33:13.447200Z",
     "iopub.status.idle": "2025-06-21T13:35:17.651685Z",
     "shell.execute_reply": "2025-06-21T13:35:17.651097Z",
     "shell.execute_reply.started": "2025-06-21T13:33:13.447752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# some random sentences from data\n",
    "\n",
    "sentences_idx = np.random.choice(sentences_en.index, size=100)\n",
    "\n",
    "actual_en = sentences_en[sentences_idx]\n",
    "actual_fr = sentences_fr[sentences_idx]\n",
    "predicted_fr = [translate_english_to_french(sentence) for sentence in actual_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T13:35:17.653006Z",
     "iopub.status.busy": "2025-06-21T13:35:17.652791Z",
     "iopub.status.idle": "2025-06-21T13:35:17.669175Z",
     "shell.execute_reply": "2025-06-21T13:35:17.668569Z",
     "shell.execute_reply.started": "2025-06-21T13:35:17.652990Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belu score: 95.29\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(predicted_fr, [actual_fr.to_list()])\n",
    "print(f\"belu score: {bleu.score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 203339,
     "sourceId": 446960,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 870709,
     "sourceId": 1483651,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
